# Quick Tour
Let's have a quick tour on some of the most important features of Hezar!

### Models
There's a bunch of ready to use trained models for different tasks on the Hub!

**ðŸ¤—Hugging Face Hub Page**: [https://huggingface.co/hezarai](https://huggingface.co/hezarai)

Let's walk you through some examples!

- **Text Classification (sentiment analysis, categorization, etc)**
```python
from hezar import Model

example = ["Ù‡Ø²Ø§Ø±ØŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒØ§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ Ú©Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¢Ø³Ø§Ù† Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"]
model = Model.load("hezarai/bert-fa-sentiment-dksf")
outputs = model.predict(example)
print(outputs)
```
```
{'labels': ['positive'], 'probs': [0.812910258769989]}
```
- **Sequence Labeling (POS, NER, etc.)**
```python
from hezar import Model

pos_model = Model.load("hezarai/bert-fa-pos-lscp-500k")  # Part-of-speech
ner_model = Model.load("hezarai/bert-fa-ner-arman")  # Named entity recognition
inputs = ["Ø´Ø±Ú©Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø²Ø§Ø±"]
pos_outputs = pos_model.predict(inputs)
ner_outputs = ner_model.predict(inputs)
print(f"POS: {pos_outputs}")
print(f"NER: {ner_outputs}")
```
```
POS: [[{'token': 'Ø´Ø±Ú©Øª', 'tag': 'Ne'}, {'token': 'Ù‡ÙˆØ´', 'tag': 'Ne'}, {'token': 'Ù…ØµÙ†ÙˆØ¹ÛŒ', 'tag': 'AJe'}, {'token': 'Ù‡Ø²Ø§Ø±', 'tag': 'NUM'}]]
NER: [[{'token': 'Ø´Ø±Ú©Øª', 'tag': 'B-org'}, {'token': 'Ù‡ÙˆØ´', 'tag': 'I-org'}, {'token': 'Ù…ØµÙ†ÙˆØ¹ÛŒ', 'tag': 'I-org'}, {'token': 'Ù‡Ø²Ø§Ø±', 'tag': 'I-org'}]]
```
- **Language Modeling**
```python
from hezar import Model

roberta_mlm = Model.load("hezarai/roberta-fa-mlm")
inputs = ["Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡ Ù‡Ø§ Ø­Ø§Ù„ØªÙˆÙ† <mask>"]
outputs = roberta_mlm.predict(inputs)
print(outputs)
```
```
{'filled_texts': ['Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡ Ù‡Ø§ Ø­Ø§Ù„ØªÙˆÙ† Ú†Ø·ÙˆØ±Ù‡'], 'filled_tokens': [' Ú†Ø·ÙˆØ±Ù‡']}
```
- **Speech Recognition**
```python
from hezar import Model

whisper = Model.load("hezarai/whisper-small-fa")
transcripts = whisper.predict("examples/assets/speech_example.mp3")
print(transcripts)
```
```
{'transcripts': ['Ùˆ Ø§ÛŒÙ† ØªÙ†Ù‡Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ Ù…Ø­ÛŒØ· Ú©Ø§Ø± Ù†ÛŒØ³Øª']}
```
- **Image to Text (OCR)**
```python
from hezar import Model
# OCR with TrOCR
model = Model.load("hezarai/trocr-base-fa-v1")
texts = model.predict(["examples/assets/ocr_example.jpg"])
print(f"TrOCR Output: {texts}")

# OCR with CRNN
model = Model.load("hezarai/crnn-base-fa-64x256")
texts = model.predict("examples/assets/ocr_example.jpg")
print(f"CRNN Output: {texts}")
```
![](https://raw.githubusercontent.com/hezarai/hezar/main/examples/assets/ocr_example.jpg)
```
TrOCR Output: {'texts': [' Ú†Ù‡ Ù…ÛŒØ´Ù‡ Ú©Ø±Ø¯ØŒ Ø¨Ø§ÛŒØ¯ ØµØ¨Ø± Ú©Ù†ÛŒÙ…']}
CRNN Output: {'texts': ['Ú†Ù‡ Ù…ÛŒØ´Ù‡ Ú©Ø±Ø¯ØŒ Ø¨Ø§ÛŒØ¯ ØµØ¨Ø± Ú©Ù†ÛŒÙ…']}
```
- **Image to Text (License Plate Recognition)**
```python
from hezar import Model

model = Model.load("hezarai/crnn-fa-64x256-license-plate-recognition")
plate_text = model.predict("assets/license_plate_ocr_example.jpg")
print(plate_text)  # Persian text of mixed numbers and characters might not show correctly in the console
```
```
{'texts': ['ÛµÛ·Ø³Û·Û·Û¹Û·Û·']}
```
![](https://raw.githubusercontent.com/hezarai/hezar/main/examples/assets/license_plate_ocr_example.jpg)

- **Image to Text (Image Captioning)**
```python
from hezar import Model

model = Model.load("hezarai/vit-roberta-fa-image-captioning-flickr30k")
texts = model.predict("examples/assets/image_captioning_example.jpg")
print(texts)
```
```
{'texts': ['Ø³Ú¯ÛŒ Ø¨Ø§ ØªÙˆÙ¾ ØªÙ†ÛŒØ³ Ø¯Ø± Ø¯Ù‡Ø§Ù†Ø´ Ù…ÛŒ Ø¯ÙˆØ¯.']}
```
![](https://raw.githubusercontent.com/hezarai/hezar/main/examples/assets/image_captioning_example.jpg)
We constantly keep working on adding and training new models and this section will hopefully be expanding over time ;)
### Word Embeddings
- **FastText**
```python
from hezar import Embedding

fasttext = Embedding.load("hezarai/fasttext-fa-300")
most_similar = fasttext.most_similar("Ù‡Ø²Ø§Ø±")
print(most_similar)
```
```
[{'score': 0.7579, 'word': 'Ù…ÛŒÙ„ÛŒÙˆÙ†'},
 {'score': 0.6943, 'word': '21Ù‡Ø²Ø§Ø±'},
 {'score': 0.6861, 'word': 'Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯'},
 {'score': 0.6825, 'word': '26Ù‡Ø²Ø§Ø±'},
 {'score': 0.6803, 'word': 'Ù£Ù‡Ø²Ø§Ø±'}]
```
- **Word2Vec (Skip-gram)**
```python
from hezar import Embedding

word2vec = Embedding.load("hezarai/word2vec-skipgram-fa-wikipedia")
most_similar = word2vec.most_similar("Ù‡Ø²Ø§Ø±")
print(most_similar)
```
```
[{'score': 0.7885, 'word': 'Ú†Ù‡Ø§Ø±Ù‡Ø²Ø§Ø±'},
 {'score': 0.7788, 'word': 'Û±Û°Ù‡Ø²Ø§Ø±'},
 {'score': 0.7727, 'word': 'Ø¯ÙˆÛŒØ³Øª'},
 {'score': 0.7679, 'word': 'Ù…ÛŒÙ„ÛŒÙˆÙ†'},
 {'score': 0.7602, 'word': 'Ù¾Ø§Ù†ØµØ¯'}]
```
- **Word2Vec (CBOW)**
```python
from hezar import Embedding

word2vec = Embedding.load("hezarai/word2vec-cbow-fa-wikipedia")
most_similar = word2vec.most_similar("Ù‡Ø²Ø§Ø±")
print(most_similar)
```
```
[{'score': 0.7407, 'word': 'Ø¯ÙˆÛŒØ³Øª'},
 {'score': 0.7400, 'word': 'Ù…ÛŒÙ„ÛŒÙˆÙ†'},
 {'score': 0.7326, 'word': 'ØµØ¯'},
 {'score': 0.7276, 'word': 'Ù¾Ø§Ù†ØµØ¯'},
 {'score': 0.7011, 'word': 'Ø³ÛŒØµØ¯'}]
```
### Datasets
You can load any of the datasets on the [Hub](https://huggingface.co/hezarai) like below:
```python
from hezar import Dataset

sentiment_dataset = Dataset.load("hezarai/sentiment-dksf")  # A TextClassificationDataset instance
lscp_dataset = Dataset.load("hezarai/lscp-pos-500k")  # A SequenceLabelingDataset instance
xlsum_dataset = Dataset.load("hezarai/xlsum-fa")  # A TextSummarizationDataset instance
...
```

### Training
Hezar makes it super easy to train models using out-of-the-box models and datasets provided in the library.
```python
from hezar import (
    BertSequenceLabeling,
    BertSequenceLabelingConfig,
    Trainer,
    TrainerConfig,
    Dataset,
    Preprocessor,
)

base_model_path = "hezarai/bert-base-fa"
dataset_path = "hezarai/lscp-pos-500k"

train_dataset = Dataset.load(dataset_path, split="train", tokenizer_path=base_model_path)
eval_dataset = Dataset.load(dataset_path, split="test", tokenizer_path=base_model_path)

model = BertSequenceLabeling(BertSequenceLabelingConfig(id2label=train_dataset.config.id2label))
preprocessor = Preprocessor.load(base_model_path)

train_config = TrainerConfig(
    task="sequence_labeling",
    device="cuda",
    init_weights_from=base_model_path,
    batch_size=8,
    num_epochs=5,
    checkpoints_dir="checkpoints/",
    metrics=["seqeval"],
)

trainer = Trainer(
    config=train_config,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=train_dataset.data_collator,
    preprocessor=preprocessor,
)
trainer.train()

trainer.push_to_hub("bert-fa-pos-lscp-500k")  # push model, config, preprocessor, trainer files and configs
```

Want to go deeper? Check out the [guides](../guide/index.md).
